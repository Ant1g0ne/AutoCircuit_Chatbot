# utils.py
from __future__ import annotations
import os
import json
import re
import streamlit as st
import pandas as pd
from fuzzywuzzy import fuzz
from openai import OpenAI

# =========================
# 1. çŸ¥è¯†åº“é…ç½®
# =========================
DATA_CSV_DEFAULT = "data/èµ„æ–™æ¸…å•.csv"

# åŒä¹‰è¯åº“ (æ ¸å¿ƒï¼šè®©æœç´¢æ‡‚è¡Œè¯)
SYNONYMS_MAP = {
    # è½¦å‹/é€šç”¨
    "2OOO": ["2000"],
    "2000": ["2OOO"],
    "å°å¿ª": ["å°æ¾"],
    "æ°å¸ˆ": ["æ°ç‹®"],
    # ç”µå™¨ç±»
    "ä¿é™©ä¸": ["ç†”æ–­å™¨", "fuse", "ä¿é™©", "é…ç”µç›’", "æ¥çº¿ç›’", "ç”µå™¨ç›’"],
    "ä¿é™©ç›’": ["ä¿é™©ä¸", "ç†”æ–­å™¨", "é…ç”µç›’"],
    "ECU": ["ç”µè„‘æ¿", "æ§åˆ¶å™¨", "ç”µæ§", "ECM", "VECU", "CBCU"],
    "ç”µè„‘æ¿": ["ECU", "æ§åˆ¶å™¨", "æ¨¡å—"],
    "ä»ªè¡¨": ["ç»„åˆä»ªè¡¨", "æ˜¾ç¤ºå±", "ç›˜"],
    "é’ˆè„š": ["ç®¡è„š", "ç«¯å­", "å®šä¹‰", "æ¥å¤´", "æ’å¤´"],
    "çº¿è·¯å›¾": ["ç”µè·¯å›¾", "åŸç†å›¾", "æ¥çº¿å›¾", "ç¤ºæ„å›¾"],
    "æ•´è½¦": ["å…¨è½¦", "ç³»ç»Ÿå›¾"],
    "ä¾›ç”µ": ["ç”µæº", "å……ç”µ", "èµ·åŠ¨"],
    "ç»ç’ƒ": ["é—¨çª—", "å‡é™å™¨"],
    "å·®é€Ÿå™¨": ["å·®é€Ÿé”", "æ¡¥"]
}

# å“ç‰Œåº“
KNOWN_BRANDS = [
    "ä¸‰ä¸€", "å¾å·¥", "ä¸œé£", "è§£æ”¾", "ä½å‹", "å°æ¾", "æ—¥ç«‹", "é›·æ²ƒ", 
    "å¡ç‰¹", "äº”åé“ƒ", "è±ªæ²ƒ", "é™•æ±½", "ç¦ç”°", "æ±Ÿæ·®", "çº¢å²©", 
    "å¤§é€š", "å®‡é€š", "é‡‘é¾™", "æ¯”äºšè¿ª", "å‰åˆ©", "é•¿åŸ", "æŸ³æ±½", "ä¹˜é¾™", "æ¬§æ›¼"
]

# ç³»åˆ—å…³é”®è¯
ALL_SERIES_KEYWORDS = [
    "å¤©é¾™", "KL", "KC", "VL", "æ——èˆ°", "å¤§åŠ›ç¥", "æ´¥å¨", 
    "J6", "J6P", "J6L", "J7", "JH6", "è™V", "é¾™V",                   
    "è±ªæ²ƒ", "T7", "TX", "æ±•å¾·å¡", "æ–¯å¤ªå°”", "è±ªç€š",                  
    "X3000", "X5000", "X6000", "M3000", "F3000", "å¾·é¾™",                      
    "ä¹˜é¾™", "H7", "H5", "M3", "éœ¸é¾™",                                       
    "SY75", "SY135", "SY215", "SY245", "SY365",              
    "ZX200", "ZX240", "4HK1", "6HK1", "2000", "3000",
    "æ°ç‹®", "æ°å¡", "é‡‘åˆš", "æ¬§æ›¼", "GTL", "EST"
]

# ç±»å‹å…³é”®è¯
ALL_TYPE_KEYWORDS = [
    "æ•´è½¦", "ECU", "ä»ªè¡¨", "åº•ç›˜", "å‘åŠ¨æœº", "ABS", "è½¦èº«", 
    "é—¨çª—", "ç¯å…‰", "ç©ºè°ƒ", "åå¤„ç†", "ä¾›ç”µ", "èµ·åŠ¨", "å……ç”µ",
    "é’ˆè„š", "çº¿è·¯å›¾", "åŸç†å›¾", "æ¥çº¿å›¾", "ä¿é™©ä¸", "ç”µè„‘æ¿", 
    "æ¥çº¿ç›’", "ç»§ç”µå™¨", "å·®é€Ÿå™¨", "ç»ç’ƒå‡é™"
]

# =========================
# 2. LLM æ¨¡å—
# =========================
def get_api_key():
    try: return st.secrets["OPENAI_API_KEY"]
    except: return os.getenv("OPENAI_API_KEY")

def llm_parse_query(query: str) -> dict:
    """GPT-4o-mini è§£æ"""
    api_key = get_api_key()
    if not api_key: return {}

    client = OpenAI(api_key=api_key)
    try:
        prompt = f"""
        Extract vehicle info.
        Role: Auto Expert.
        Rules:
        1. Correct typos ("å°å¿ª"->"å°æ¾", "2ooo"->"2000").
        2. "çº¢å²©æ°ç‹®" -> Brand:çº¢å²©, Series:æ°ç‹®.
        Return JSON: {{"brand": "...", "series": "...", "part": "..."}}
        Query: {query}
        """
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0
        )
        txt = resp.choices[0].message.content.strip()
        match = re.search(r'\{.*\}', txt, re.DOTALL)
        return json.loads(match.group(0)) if match else {}
    except:
        return {}

# =========================
# 3. æ•°æ®ä¸æœç´¢ (å¢å¼ºç‰ˆ)
# =========================
def _read_csv_robust(path):
    for enc in ["utf-8-sig", "utf-8", "gbk", "gb18030"]:
        try: return pd.read_csv(path, encoding=enc)
        except: continue
    raise ValueError("æ— æ³•è¯»å–CSV")

def load_data(path=DATA_CSV_DEFAULT):
    if not os.path.exists(path): return pd.DataFrame()
    df = _read_csv_robust(path)
    df.columns = df.columns.astype(str).str.strip()
    df["å±‚çº§è·¯å¾„"] = df["å±‚çº§è·¯å¾„"].astype(str).fillna("")
    df["å…³è”æ–‡ä»¶åç§°"] = df["å…³è”æ–‡ä»¶åç§°"].astype(str).fillna("")
    df["_search_blob"] = (df["å…³è”æ–‡ä»¶åç§°"] + " " + df["å±‚çº§è·¯å¾„"]).astype(str)
    return df

def get_expanded_keywords(keyword: str) -> list[str]:
    """åŒä¹‰è¯æ‰©å±•"""
    keywords = [keyword]
    for k, v in SYNONYMS_MAP.items():
        if keyword.lower() == k.lower():
            keywords.extend(v)
        elif keyword.lower() in [x.lower() for x in v]:
            keywords.append(k)
            keywords.extend([x for x in v if x.lower() != keyword.lower()])
    return list(set(keywords))

def check_text_matches_any(text: str, keywords: list[str]) -> bool:
    text_u = text.upper()
    for k in keywords:
        if k.upper() in text_u:
            return True
    return False

def search_topk(df, query, k=200):
    if not query: return pd.DataFrame()
    df_copy = df.copy()
    
    def calculate_score(text):
        return fuzz.partial_token_set_ratio(query, str(text))

    df_copy["_score"] = df_copy["_search_blob"].apply(calculate_score)
    # é˜ˆå€¼40ï¼Œä¿è¯å¬å›
    return df_copy[df_copy["_score"] >= 40].sort_values("_score", ascending=False).head(k)

# =========================
# 4. é€‰é¡¹æ£€æµ‹ (ä¿®æ­£å‡½æ•°å)
# =========================
def detect_options(current_df: pd.DataFrame, keyword_list: list) -> list[str]:
    """ğŸ”¥ ä¿®å¤ï¼šå‡½æ•°åç»Ÿä¸€ä¸º detect_options"""
    if current_df.empty: return []
    text_blob = " ".join(current_df.head(100)["_search_blob"].astype(str).tolist()).upper()
    found = []
    for k in keyword_list:
        if k.upper() in text_blob:
            found.append(k)
    return found